use_gpu: True
device: -1
seed: 1

early_stop:
    patience: 5

federate:
    mode: standalone
    local_update_steps: 1
    batch_or_epoch: epoch
    total_round_num: 500
    client_num: 50
    sample_client_rate: 0.2
    unseen_clients_rate: 0.2

data:
    root: 'glue'
    type: 'sst2@huggingface_datasets'
    args: [{
        #'hg_cache_dir': 'huggingface', 
        #'load_disk_dir': 'huggingface/datasets/glue/sst2', 
        'max_len': 128, 
        'part_train_dummy_val': 0.2,
        'val_as_dummy_test': true
    }]
    batch_size: 64
    splitter: 'lda'
    splitter_args: [{'alpha': 0.4}]
    num_workers: 0

model:
    type: 'google/bert_uncased_L-2_H-128_A-2@transformers'
    task: 'SequenceClassification'
    out_channels: 2

optimizer:
    lr: 0.3
    weight_decay: 0.0
    grad_clip: 5.0

criterion:
    type: CrossEntropyLoss

trainer:
    type: nlptrainer

eval:
    freq: 5
    metrics: ['acc', 'correct', 'f1']
