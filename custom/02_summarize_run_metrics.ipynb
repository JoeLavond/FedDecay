{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import packages and processed run metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('final_run_metrics.csv')\n",
    "\n",
    "validation_metric = 'Results/val_acc'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter data to runs of interest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "filtered_df = df.loc[(df.n_epochs < 6)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write function to return summaries of run's metric performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# get best runs for each group\n",
    "idx = filtered_df.groupby(['dataset', 'method', 'finetune']) \\\n",
    "    [validation_metric].idxmax()  # return index of max validation metric\n",
    "filtered_df = df.loc[idx]\n",
    "\n",
    "## Process metrics and get top runs for each\n",
    "# are large or small metric values are desirable?\n",
    "descending_metrics = [name for name in filtered_df.columns if re.match('Results', name)]\n",
    "ascending_metrics = [\n",
    "    descending_metrics.pop(descending_metrics.index(name))\n",
    "    for name in descending_metrics\n",
    "    if re.search('std', name)\n",
    "]\n",
    "\n",
    "# all non-metrics columns are used to identify the experimental run\n",
    "filtered_runs = filtered_df[[\n",
    "    name for name in filtered_df.columns\n",
    "    if name not in descending_metrics + ascending_metrics\n",
    "]]\n",
    "\n",
    "# rank the metrics\n",
    "ranked_descending = filtered_df[descending_metrics].rank(\n",
    "    method='first',\n",
    "    ascending=False\n",
    ")\n",
    "ranked_ascending = filtered_df[ascending_metrics].rank(\n",
    "    method='first',\n",
    "    ascending=True\n",
    ")\n",
    "\n",
    "# combine and sort the ranked_metrics\n",
    "ranked_metrics = pd.concat([ranked_descending, ranked_ascending], axis=1)\n",
    "ranked_metrics = ranked_metrics[sorted(ranked_metrics.columns)]\n",
    "\n",
    "id_columns = ['dataset', 'method', 'finetune']\n",
    "filtered_ranks = filtered_runs[id_columns].join(ranked_metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   dataset  method  finetune  rank_one_count  rank_two_count  \\\n28    sst2   ditto     False               4               0   \n48    sst2   ditto      True               2               2   \n3     sst2   exact     False               4               9   \n79    sst2   exact      True               0               1   \n12    sst2  fedavg     False               1               0   \n\n    rank_three_count  top_rank_count  \n28                 1               5  \n48                 0               4  \n3                  5              18  \n79                 4               5  \n12                 3               4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>method</th>\n      <th>finetune</th>\n      <th>rank_one_count</th>\n      <th>rank_two_count</th>\n      <th>rank_three_count</th>\n      <th>top_rank_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>28</th>\n      <td>sst2</td>\n      <td>ditto</td>\n      <td>False</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>sst2</td>\n      <td>ditto</td>\n      <td>True</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sst2</td>\n      <td>exact</td>\n      <td>False</td>\n      <td>4</td>\n      <td>9</td>\n      <td>5</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>sst2</td>\n      <td>exact</td>\n      <td>True</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>sst2</td>\n      <td>fedavg</td>\n      <td>False</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Compute rank summaries to understand what runs are top overall\n",
    "rank_ones = (ranked_metrics == 1).sum(axis = 1)\n",
    "rank_twos = (ranked_metrics == 2).sum(axis = 1)\n",
    "rank_threes = (ranked_metrics == 3).sum(axis = 1)\n",
    "rank_summaries = pd.concat({\n",
    "    'rank_one_count':rank_ones,\n",
    "    'rank_two_count':rank_twos,\n",
    "    'rank_three_count':rank_threes\n",
    "}, axis=1)\n",
    "\n",
    "# append summary to run id columns\n",
    "filtered_summary = filtered_runs[id_columns].join(rank_summaries)\n",
    "\n",
    "# sort based on runs and view\n",
    "filtered_summary = filtered_summary.sort_values(by=id_columns)\n",
    "filtered_summary['top_rank_count'] = filtered_summary[[name for name in filtered_summary.columns if name not in id_columns]].sum(axis=1)\n",
    "filtered_summary.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                  F    T\nmetric                                  \nResults/test_acc                2.0  1.0\nResults/test_acc_bottom_decile  1.0  2.0\nResults/test_acc_std            1.0  2.0\nResults/test_f1                 2.0  1.0\nResults/test_f1_bottom_decile   2.0  1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F</th>\n      <th>T</th>\n    </tr>\n    <tr>\n      <th>metric</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Results/test_acc</th>\n      <td>2.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Results/test_acc_bottom_decile</th>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>Results/test_acc_std</th>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>Results/test_f1</th>\n      <td>2.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Results/test_f1_bottom_decile</th>\n      <td>2.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find metrics where decay performs well for each dataset\n",
    "# convert to long format\n",
    "# filter to top ranks\n",
    "long_filtered_ranks = pd.melt(filtered_ranks, id_vars=id_columns, var_name='metric')\n",
    "top_filtered_metrics = long_filtered_ranks.loc[long_filtered_ranks.value <= 3].copy()  # top 3 runs\n",
    "\n",
    "# check which metrics result in top runs for decay vs. other\n",
    "top_filtered_metrics['decay'] = (top_filtered_metrics.method == 'exact')\n",
    "filter_columns = ['metric', 'decay']\n",
    "top_filtered_metrics = top_filtered_metrics.groupby(filter_columns).count()\n",
    "top_filtered_metrics = top_filtered_metrics.reset_index()[filter_columns + ['value']]\n",
    "\n",
    "# convert back to wide format\n",
    "top_filtered_metrics = top_filtered_metrics.pivot(index='metric', columns='decay')\n",
    "print(top_filtered_metrics.shape)\n",
    "top_filtered_metrics.columns = [str(col[-1])[0] for col in top_filtered_metrics.columns]\n",
    "top_filtered_metrics.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\n    # get best runs for each group\\n    idx = df.groupby(['dataset', 'method', 'finetune'])         [validation_metric].idxmax()  # return index of max validation metric\\n    filtered_df = df.loc[idx]\\n\\n\\n    ## Process metrics and get top runs for each\\n    # are large or small metric values are desirable?\\n    descending_metrics = [name for name in filtered_df.columns if re.match('Results', name)]\\n    ascending_metrics = [\\n        descending_metrics.pop(descending_metrics.index(name))\\n        for name in descending_metrics\\n        if re.search('std', name)\\n    ]\\n\\n    # all non-metrics columns are used to identify the experimental run\\n    filtered_runs = filtered_df[[\\n        name for name in filtered_df.columns\\n        if name not in descending_metrics + ascending_metrics\\n    ]]\\n\\n    # rank the metrics\\n    ranked_descending = filtered_df[descending_metrics].rank(\\n        method='first',\\n        ascending=False\\n    )\\n    ranked_ascending = filtered_df[ascending_metrics].rank(\\n        method='first',\\n        ascending=True\\n    )\\n\\n    # combine and sort the ranked_metrics\\n    ranked_metrics = pd.concat([ranked_descending, ranked_ascending], axis=1)\\n    ranked_metrics = ranked_metrics[sorted(ranked_metrics.columns)]\\n    filtered_ranks = filtered_runs.join(ranked_metrics)\\n\\n\\n    ## Compute rank summaries to understand what runs are top overall\\n    rank_ones = (ranked_metrics == 1).sum(axis = 1)\\n    rank_twos = (ranked_metrics == 2).sum(axis = 1)\\n    rank_threes = (ranked_metrics == 3).sum(axis = 1)\\n    rank_summaries = pd.concat({\\n        'rank_one_count':rank_ones,\\n        'rank_two_count':rank_twos,\\n        'rank_three_count':rank_threes\\n    }, axis=1)\\n    print(rank_summaries)\\n\\n    # append summary to run id columns\\n    id_columns = ['dataset', 'method', 'finetune']\\n    filtered_summary = filtered_runs[id_columns].join(rank_summaries)\\n\\n    # sort based on runs and view\\n    filtered_summary = filtered_summary.sort_values(by=id_columns)\\n    filtered_summary.head()\\n\\n    return (\\n        filtered_ranks,\\n        filtered_df\\n    )\\n\""
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_run_metrics(df):\n",
    "    pass\n",
    "\n",
    "\"\"\"\n",
    "    # get best runs for each group\n",
    "    idx = df.groupby(['dataset', 'method', 'finetune']) \\\n",
    "        [validation_metric].idxmax()  # return index of max validation metric\n",
    "    filtered_df = df.loc[idx]\n",
    "\n",
    "\n",
    "    ## Process metrics and get top runs for each\n",
    "    # are large or small metric values are desirable?\n",
    "    descending_metrics = [name for name in filtered_df.columns if re.match('Results', name)]\n",
    "    ascending_metrics = [\n",
    "        descending_metrics.pop(descending_metrics.index(name))\n",
    "        for name in descending_metrics\n",
    "        if re.search('std', name)\n",
    "    ]\n",
    "\n",
    "    # all non-metrics columns are used to identify the experimental run\n",
    "    filtered_runs = filtered_df[[\n",
    "        name for name in filtered_df.columns\n",
    "        if name not in descending_metrics + ascending_metrics\n",
    "    ]]\n",
    "\n",
    "    # rank the metrics\n",
    "    ranked_descending = filtered_df[descending_metrics].rank(\n",
    "        method='first',\n",
    "        ascending=False\n",
    "    )\n",
    "    ranked_ascending = filtered_df[ascending_metrics].rank(\n",
    "        method='first',\n",
    "        ascending=True\n",
    "    )\n",
    "\n",
    "    # combine and sort the ranked_metrics\n",
    "    ranked_metrics = pd.concat([ranked_descending, ranked_ascending], axis=1)\n",
    "    ranked_metrics = ranked_metrics[sorted(ranked_metrics.columns)]\n",
    "    filtered_ranks = filtered_runs.join(ranked_metrics)\n",
    "\n",
    "\n",
    "    ## Compute rank summaries to understand what runs are top overall\n",
    "    rank_ones = (ranked_metrics == 1).sum(axis = 1)\n",
    "    rank_twos = (ranked_metrics == 2).sum(axis = 1)\n",
    "    rank_threes = (ranked_metrics == 3).sum(axis = 1)\n",
    "    rank_summaries = pd.concat({\n",
    "        'rank_one_count':rank_ones,\n",
    "        'rank_two_count':rank_twos,\n",
    "        'rank_three_count':rank_threes\n",
    "    }, axis=1)\n",
    "    print(rank_summaries)\n",
    "\n",
    "    # append summary to run id columns\n",
    "    id_columns = ['dataset', 'method', 'finetune']\n",
    "    filtered_summary = filtered_runs[id_columns].join(rank_summaries)\n",
    "\n",
    "    # sort based on runs and view\n",
    "    filtered_summary = filtered_summary.sort_values(by=id_columns)\n",
    "    filtered_summary.head()\n",
    "\n",
    "    return (\n",
    "        filtered_ranks,\n",
    "        filtered_df\n",
    "    )\n",
    "\"\"\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
