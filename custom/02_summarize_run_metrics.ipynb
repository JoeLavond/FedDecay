{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import packages and processed run metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input datasets:\n",
      "\t (1200, 35)\n",
      "\t (780, 28)\n",
      "\t (520, 27)\n",
      "\t (520, 27)\n",
      "\t (520, 27)\n",
      "all runs: (3540, 36)\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# control\n",
    "project_names = [\n",
    "    'decay--sst2',\n",
    "    'decay--pubmed',\n",
    "    'decay--cifar--alpha5.0',\n",
    "    'decay--cifar--alpha0.5',\n",
    "    'decay--cifar--alpha0.1'\n",
    "]\n",
    "validation_metric = 'Results/val_acc'\n",
    "\n",
    "# read each data set\n",
    "df = []\n",
    "print('input datasets:')\n",
    "for project_name in project_names:\n",
    "    temp = pd.read_csv(f'{project_name}.csv')\n",
    "    print('\\t', temp.shape)\n",
    "    df.append(temp)\n",
    "\n",
    "df = pd.concat(df, axis=0, ignore_index=True)\n",
    "print('all runs:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write function to return summaries of run's metric performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def process_run_metrics(filtered_df, method='exact'):\n",
    "    pass\n",
    "\n",
    "\n",
    "    ## Process metrics and get top runs for each\n",
    "    # are large or small metric values are desirable?\n",
    "    descending_metrics = [name for name in filtered_df.columns if re.search('Results.*test', name)]\n",
    "    ascending_metrics = [\n",
    "        descending_metrics.pop(descending_metrics.index(name))\n",
    "        for name in descending_metrics\n",
    "        if re.search('std', name)\n",
    "    ]\n",
    "    print('metrics:')\n",
    "    print('\\t', descending_metrics)\n",
    "    print('\\t', ascending_metrics)\n",
    "\n",
    "    # convert metrics to numeric\n",
    "    #filtered_df[descending_metrics + ascending_metrics] = filtered_df[descending_metrics + ascending_metrics].apply(pd.to_numeric)\n",
    "\n",
    "    # all non-metrics columns are used to identify the experimental run\n",
    "    filtered_runs = filtered_df[[\n",
    "        name for name in filtered_df.columns\n",
    "        if name not in descending_metrics + ascending_metrics\n",
    "    ]]\n",
    "\n",
    "    # rank the metrics\n",
    "    ranked_descending = filtered_df.groupby(['dataset'])[descending_metrics].rank(\n",
    "        method='min',\n",
    "        ascending=False\n",
    "    )\n",
    "    ranked_ascending = filtered_df.groupby(['dataset'])[ascending_metrics].rank(\n",
    "        method='min',\n",
    "        ascending=True\n",
    "    )\n",
    "\n",
    "    # combine and sort the ranked_metrics\n",
    "    ranked_metrics = pd.concat([ranked_descending, ranked_ascending], axis=1)\n",
    "    ranked_metrics = ranked_metrics[sorted(ranked_metrics.columns)]\n",
    "    filtered_ranks = filtered_runs.join(ranked_metrics)\n",
    "\n",
    "\n",
    "    ## Manipulate rank data to be summarized by runs and metrics\n",
    "    # convert to long format\n",
    "    # filter to top ranks\n",
    "    long_filtered_ranks = pd.melt(filtered_ranks, id_vars=filtered_runs.columns, var_name='metric')\n",
    "    top_filtered_metrics = long_filtered_ranks.loc[long_filtered_ranks.value <= 3].copy()  # top 3 runs\n",
    "\n",
    "    # compute rank summaries to understand what runs are top overall\n",
    "    top_filtered_metrics['rank_one_ind'] = (top_filtered_metrics.value == 1)\n",
    "    top_filtered_metrics['rank_two_ind'] = (top_filtered_metrics.value == 2)\n",
    "    top_filtered_metrics['rank_three_ind'] = (top_filtered_metrics.value == 3)\n",
    "    top_filtered_metrics.replace(False, pd.NA, inplace=True)\n",
    "\n",
    "    # summarized metric ranks for run type\n",
    "    rank_summary_columns = ['rank_one_ind', 'rank_two_ind', 'rank_three_ind', 'value']\n",
    "    #id_columns = ['dataset', 'method', 'finetune']\n",
    "    id_columns = ['method', 'finetune']\n",
    "    run_summary = top_filtered_metrics.groupby(id_columns)[rank_summary_columns].count()\n",
    "\n",
    "    # summarize metric ranks for metric choice\n",
    "    metric_summary = top_filtered_metrics.loc[top_filtered_metrics.method == method]\n",
    "    metric_summary = metric_summary.sort_values(by='metric').groupby('metric')[rank_summary_columns].count()\n",
    "\n",
    "    return (\n",
    "        run_summary,\n",
    "        metric_summary,\n",
    "        filtered_runs\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter data to runs of interest\n",
    "Apply summary function to filtered dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0      NaN\n1      NaN\n2      NaN\n3      NaN\n4      NaN\n        ..\n3535   NaN\n3536   NaN\n3537   NaN\n3538   NaN\n3539   NaN\nName: K, Length: 3540, dtype: float64"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.K"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dataset: (3540, 36)\n",
      "\t reduce to 3 or fewer local update steps: (3540, 36)\n",
      "\t reduce to 3 or fewer local meta-learning steps for pfedme: (3540, 36)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m subset_df \u001B[38;5;241m=\u001B[39m subset_df\u001B[38;5;241m.\u001B[39mloc[(df\u001B[38;5;241m.\u001B[39mK \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m6\u001B[39m) \u001B[38;5;241m|\u001B[39m (df\u001B[38;5;241m.\u001B[39mmethod \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpfedme\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m reduce to 3 or fewer local meta-learning steps for pfedme:\u001B[39m\u001B[38;5;124m'\u001B[39m, df\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 10\u001B[0m subset_df \u001B[38;5;241m=\u001B[39m subset_df\u001B[38;5;241m.\u001B[39mloc[(\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbeta\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m|\u001B[39m (df\u001B[38;5;241m.\u001B[39mmethod \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexact\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m reduce beta grid for decay:\u001B[39m\u001B[38;5;124m'\u001B[39m, df\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;124;03msubset_df = subset_df.loc[[\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m    True if beta in betas else False\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;124;03m]]\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\modified-pfl-bench\\venv\\lib\\site-packages\\pandas\\core\\series.py:206\u001B[0m, in \u001B[0;36m_coerce_method.<locals>.wrapper\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m converter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m--> 206\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot convert the series to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconverter\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: cannot convert the series to <class 'int'>"
     ]
    }
   ],
   "source": [
    "subset_df = df\n",
    "print('input dataset:', df.shape)\n",
    "\n",
    "## row (run) filtering\n",
    "# remove extra hyper-parameter searches\n",
    "subset_df = subset_df.loc[(df.n_epochs < 6)]\n",
    "print('\\t reduce to 3 or fewer local update steps:', df.shape)\n",
    "subset_df = subset_df.loc[(df.K < 6) | (df.method != 'pfedme')]\n",
    "print('\\t reduce to 3 or fewer local meta-learning steps for pfedme:', df.shape)\n",
    "subset_df = subset_df.loc[(int(10 * df.beta) % 2 == 0) | (df.method != 'exact')]\n",
    "print('\\t reduce beta grid for decay:', df.shape)\n",
    "\n",
    "\"\"\"\n",
    "subset_df = subset_df.loc[[\n",
    "    True if beta in betas else False\n",
    "    for beta in subset_df.beta\n",
    "    ] | (subset_df.method != 'exact')\n",
    "]\n",
    "\n",
    "# column (metric) filtering\n",
    "subset_df = subset_df[[\n",
    "    name for name in subset_df.columns\n",
    "    if not re.search('f1', name)\n",
    "       and not re.search('loss', name)\n",
    "]]\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## get best runs for each group\n",
    "# regardless of finetuning\n",
    "best_runs = df.loc[\n",
    "    df.groupby(['dataset', 'method', 'finetune']) \\\n",
    "    [validation_metric].idxmax()  # return index of max validation metric\n",
    "]\n",
    "\n",
    "# treat finetuning groups as seperate\n",
    "ft_yes = best_runs.loc[best_runs.finetune == 1]\n",
    "ft_no = best_runs.loc[best_runs.finetune == 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For all best runs, regardless of finetuning, produce summaries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(all_run_summary, all_metric_summary, _) = process_run_metrics(best_runs)\n",
    "print('all runs:', all_run_summary.shape)\n",
    "print('all metrics:', all_metric_summary.shape)\n",
    "\n",
    "# run summary\n",
    "all_run_summary.sort_values(by='value', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "View processed summaries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# metrics summary\n",
    "sort_by = ['value', 'rank_one_ind', 'rank_two_ind', 'rank_three_ind']\n",
    "all_metric_summary.sort_values(by=sort_by, ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now seperately, based on finetuning status, repeat the above summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(yes_run_summary, yes_metric_summary, _) = process_run_metrics(ft_yes)\n",
    "print('ft_yes runs:', yes_run_summary.shape)\n",
    "print('ft_yes metrics:', yes_metric_summary.shape)\n",
    "(no_run_summary, no_metric_summary, _) = process_run_metrics(ft_no)\n",
    "print('ft_no runs:', all_run_summary.shape)\n",
    "print('ft_no metrics:', all_metric_summary.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run summary\n",
    "yes_run_summary.sort_values(by='value', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# metrics summary\n",
    "yes_metric_summary.sort_values(by=sort_by, ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run summary\n",
    "no_run_summary.sort_values(by='value', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# metrics summary\n",
    "no_metric_summary.sort_values(by=sort_by, ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
