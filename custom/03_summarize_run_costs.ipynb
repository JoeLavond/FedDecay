{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Import packages and processed run metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def df_to_latex(\n",
    "    df,\n",
    "    id_columns,\n",
    "    file_name,\n",
    "    remove_columns=None,\n",
    "    file_path='output'\n",
    "):\n",
    "\n",
    "    # remove columns not of interest\n",
    "    if remove_columns is None:\n",
    "        remove_columns = list()\n",
    "    temp_df = df[[name for name in df.columns if name not in remove_columns]]\n",
    "\n",
    "    # sort by id columns\n",
    "    temp_df = temp_df.sort_values(by=id_columns)\n",
    "\n",
    "    # create output file\n",
    "    with open(os.path.join(file_path, f'{file_name}.txt'), 'w') as f:\n",
    "        string_df = temp_df.to_string(\n",
    "            header=True,\n",
    "            index=False,\n",
    "            index_names=False\n",
    "        )\n",
    "\n",
    "        # remove any leading space\n",
    "        string_df = re.sub('^[\\s]+', '', string_df)\n",
    "        # remove leading space after newlines\n",
    "        string_df = re.sub('\\n[\\s]+', '\\n', string_df)\n",
    "        # replace white space between words with table column skip\n",
    "        string_df = re.sub('[ \\t]+', ' & ', string_df)\n",
    "\n",
    "        # add latex newline to end of each line\n",
    "        string_df = string_df.replace('_', ' ')\n",
    "        string_df = string_df.replace('\\n', ' \\\\\\\\\\n')\n",
    "\n",
    "        # write to file\n",
    "        f.writelines(string_df + ' \\\\\\\\')\n",
    "\n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T17:14:56.039174Z",
     "end_time": "2023-04-05T17:14:56.107152Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-05T17:14:56.057021Z",
     "end_time": "2023-04-05T17:14:56.167984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input datasets:\n",
      "\t (413, 35)\n",
      "\t (1200, 44)\n",
      "\t (780, 37)\n",
      "all runs: (2393, 44)\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# source\n",
    "helper = __import__('00_helper')\n",
    "\n",
    "# control\n",
    "project_names = [\n",
    "    'femnist--s02', 'sst2', 'pubmed',\n",
    "    #'cifar--alpha5.0', 'cifar--alpha0.5', 'cifar--alpha0.1'\n",
    "]\n",
    "validation_metric = 'Results/val_acc'\n",
    "\n",
    "df = helper.load_data(project_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter data to runs of interest\n",
    "Apply summary function to filtered dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dataset: (2393, 44)\n",
      "\t reduce to 3 or fewer local update steps: (1787, 44)\n",
      "\t reduce to 3 or fewer local meta-learning steps for pfedme: (1707, 44)\n",
      "\t reduce beta grid for decay: (1610, 44)\n"
     ]
    }
   ],
   "source": [
    "subset_df = df\n",
    "print('input dataset:', df.shape)\n",
    "\n",
    "## row (run) filtering\n",
    "# remove extra hyper-parameter searches\n",
    "\n",
    "subset_df = subset_df.loc[(subset_df.n_epochs < 6)]\n",
    "print('\\t reduce to 3 or fewer local update steps:', subset_df.shape)\n",
    "\n",
    "subset_df = subset_df.loc[(subset_df.K < 6) | (subset_df.method != 'pfedme')]\n",
    "print('\\t reduce to 3 or fewer local meta-learning steps for pfedme:', subset_df.shape)\n",
    "\n",
    "subset_df = subset_df.loc[((10 * subset_df.beta).astype('Int64') % 2 == 0) | (subset_df.method != 'exact')]\n",
    "print('\\t reduce beta grid for decay:', subset_df.shape)\n",
    "\n",
    "## column (metric) filtering\n",
    "subset_df = subset_df[[\n",
    "    name for name in subset_df.columns\n",
    "    if (\n",
    "        not re.search('test', name)\n",
    "        and not re.search('f1', name)\n",
    "        and not re.search('loss', name)\n",
    "    )\n",
    "]]\n",
    "\n",
    "def convert_SI(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "\n",
    "    SI = {\n",
    "        'K': 10e3,\n",
    "        'M': 10e6,\n",
    "        'G': 10e9,\n",
    "        'T': 10e12\n",
    "    }\n",
    "\n",
    "    units = re.sub('[0-9.]', '', x)\n",
    "    x = float(re.sub('[^0-9.]', '', x))\n",
    "\n",
    "    if units == '':\n",
    "        return x\n",
    "    return x * SI[units]\n",
    "\n",
    "\n",
    "def to_SI(x, unit='G', decimals=2):\n",
    "    SI = {\n",
    "        'K': 10e3, 'M': 10e6,\n",
    "        'G': 10e9, 'T': 10e12\n",
    "    }\n",
    "\n",
    "    assert unit in [name for name, _ in SI.items()]\n",
    "    return str(round(x / SI[unit], decimals)) + f' {unit}'\n",
    "\n",
    "\n",
    "#convert_SI('19.0M')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T17:14:56.173453Z",
     "end_time": "2023-04-05T17:14:56.275799Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['12.53M', '6.27M', '18.8M', nan, '4.1M', '12.3M', '8.2M', '33.45K',\n       '66.9K', '100.35K'], dtype=object)"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_df['sys_avg/total_model_size'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T17:14:56.206521Z",
     "end_time": "2023-04-05T17:14:56.311469Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (1610, 20)\n",
      "after: (102, 20)\n",
      "metrics: ['sys_avg/global_convergence_time_minutes', 'sys_avg/total_model_size', 'sys_avg/fl_end_time_minutes', 'sys_avg/total_flops', 'sys_avg/local_convergence_time_minutes', 'sys_avg/local_convergence_round', 'sys_avg/global_convergence_round', 'sys_avg/total_upload_bytes', 'sys_avg/total_download_bytes']\n",
      "without FT: (51, 20)\n"
     ]
    }
   ],
   "source": [
    "## get best runs for each group\n",
    "top_k = 3\n",
    "print('before:', subset_df.shape)\n",
    "best_runs = subset_df \\\n",
    "    .sort_values(by=validation_metric, ascending=False) \\\n",
    "    .groupby(['dataset', 'method', 'finetune']) \\\n",
    "    .head(top_k)\n",
    "print('after:', best_runs.shape)\n",
    "\n",
    "# columns of interest\n",
    "id_columns = ['dataset', 'method']\n",
    "metrics = [name for name in best_runs.columns if re.search('^sys', name)]\n",
    "print('metrics:', metrics)\n",
    "object_metrics= [\n",
    "    name for name, types in zip(best_runs.columns, best_runs.dtypes)\n",
    "    if re.search('^sys', name)\n",
    "    and types == 'object'\n",
    "]\n",
    "\n",
    "# subset to non-fine-tuned models\n",
    "float_df = best_runs.loc[best_runs.finetune == 0]\n",
    "print('without FT:', float_df.shape)\n",
    "float_df = float_df[id_columns + metrics]\n",
    "float_df[object_metrics] = float_df[object_metrics].applymap(convert_SI).apply(pd.to_numeric)\n",
    "float_df['sys_avg/total_bytes'] = float_df['sys_avg/total_download_bytes'] \\\n",
    "                          + float_df['sys_avg/total_upload_bytes']\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T17:14:56.231637Z",
     "end_time": "2023-04-05T17:14:56.312696Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "sub_metrics = [\n",
    "    'sys_avg/global_convergence_round',\n",
    "    'sys_avg/global_convergence_time_minutes',\n",
    "    'sys_avg/total_bytes',\n",
    "    'sys_avg/total_flops'\n",
    "]\n",
    "avg_runs = float_df.groupby(id_columns)[sub_metrics].mean()\n",
    "\n",
    "avg_runs['sys_avg/global_convergence_round'] = avg_runs['sys_avg/global_convergence_round'].apply(round, ndigits=2)\n",
    "avg_runs['sys_avg/global_convergence_time_minutes'] = avg_runs['sys_avg/global_convergence_time_minutes'].apply(round, ndigits=2)\n",
    "avg_runs['sys_avg/total_bytes'] = avg_runs['sys_avg/total_bytes'].apply(to_SI, unit='M')\n",
    "avg_runs['sys_avg/total_flops'] = avg_runs['sys_avg/total_flops'].apply(to_SI, unit='G')\n",
    "avg_runs = avg_runs.add_suffix('_mean').reset_index()\n",
    "\n",
    "id_columns = ['method', 'dataset']\n",
    "avg_runs.columns = [re.sub('sys_avg/', '', name) for name in avg_runs.columns]\n",
    "df_to_latex(avg_runs, id_columns, 'method_costs--ft_no')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T17:14:56.276904Z",
     "end_time": "2023-04-05T17:14:56.377020Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
